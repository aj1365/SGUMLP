{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8MezWv5S2D1pDAMGMul55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/SGUMLP/blob/main/SGUMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvw1Gbijv5Vn"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from plotly.offline import init_notebook_mode\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## You need to modify the output data here depending on the dataset\n",
        "\n",
        "def loadData(name):\n",
        "\n",
        "    data_path = os.path.join(os.getcwd(),'E:/Datasets/')\n",
        "\n",
        "    if name == 'Houston':\n",
        "\n",
        "        MS = sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/data_MS_HR.mat'))['data_MS_HR']\n",
        "        HS= sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/data_HS_LR.mat'))['data_HS_LR']\n",
        "        Train = sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/Train.mat'))['Train']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'HS-MS Houston2013/Validation.mat'))['Validation']\n",
        "\n",
        "    if name == 'Berlin':\n",
        "\n",
        "        SAR = sio.loadmat(os.path.join(data_path, 'HS-SAR Berlin/data_SAR_HR.mat'))['data_SAR_HR']\n",
        "        HS = sio.loadmat(os.path.join(data_path, 'HS-SAR Berlin/data_HS_LR.mat'))['data_HS_LR']\n",
        "        Train= sio.loadmat(os.path.join(data_path, 'HS-SAR Berlin/TrainImage.mat'))['TrainImage']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'HS-SAR Berlin/TestImage.mat'))['TestImage']\n",
        "\n",
        "    if name == 'Augsburg':\n",
        "\n",
        "        SAR = sio.loadmat(os.path.join(data_path, 'HS-SAR-DSM Augsburg/data_SAR_HR.mat'))['data_SAR_HR']\n",
        "        HS = sio.loadmat(os.path.join(data_path, 'HS-SAR-DSM Augsburg/data_HS_LR.mat'))['data_HS_LR']\n",
        "        DSM = sio.loadmat(os.path.join(data_path, 'HS-SAR-DSM Augsburg/data_DSM.mat'))['data_DSM']\n",
        "        Train= sio.loadmat(os.path.join(data_path, 'HS-SAR-DSM Augsburg/TrainImage.mat'))['TrainImage']\n",
        "        Test = sio.loadmat(os.path.join(data_path, 'HS-SAR-DSM Augsburg/TestImage.mat'))['TestImage']\n",
        "\n",
        "    return SAR, HS, DSM, Train, Test\n"
      ],
      "metadata": {
        "id": "82wnpzx0wDp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GLOBAL VARIABLES\n",
        "windowSize = 9\n",
        "\n",
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca\n",
        "\n",
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "def createImageCubes(X, y, windowSize=9, removeZeroLabels = True):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "metadata": {
        "id": "RFYTIIGywDtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'Augsburg'\n",
        "\n",
        "SAR, HS , DSM, Train, Test = loadData(dataset)\n",
        "\n",
        "K = 15 if dataset == 'Berlin' else 12\n",
        "HSr, pca = applyPCA(HS, numComponents=K)\n",
        "\n",
        "DSM= DSM.reshape((DSM.shape[0],DSM.shape[1],1))"
      ],
      "metadata": {
        "id": "qLHzaatGwDwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XTrain1, Train1 = createImageCubes(SAR, Train, windowSize=windowSize)\n",
        "XTest1, Test1 = createImageCubes(SAR, Test, windowSize=windowSize)\n",
        "\n",
        "\n",
        "XTrain2, Train2 = createImageCubes(HSr, Train, windowSize=windowSize)\n",
        "XTest2, Test2 = createImageCubes(HSr, Test, windowSize=windowSize)\n",
        "\n",
        "\n",
        "XTrain3, Train3 = createImageCubes(DSM, Train, windowSize=windowSize)\n",
        "XTest3, Test3 = createImageCubes(DSM, Test, windowSize=windowSize)\n",
        "\n",
        "XtrainS = np.concatenate((XTrain1, XTrain2, XTrain3) , axis = 3)\n",
        "XtestS = np.concatenate((XTest1, XTest2, XTest3) , axis = 3)\n",
        "\n",
        "YtestS = Train1\n",
        "YtrainS = Test1\n",
        "num_classes = 7\n",
        "input_shape = (9, 9, 17)"
      ],
      "metadata": {
        "id": "SSlaW1wvwDyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size=9\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(XtrainS)"
      ],
      "metadata": {
        "id": "5YtgX0cxwD1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_cv_attention_models.backend import layers, models, functional, initializers\n",
        "\n",
        "from keras_cv_attention_models.attention_layers import (\n",
        "    ChannelAffine,\n",
        "    CompatibleExtractPatches,\n",
        "    depthwise_conv2d_no_bias,\n",
        "    conv2d_no_bias,\n",
        "    drop_block,\n",
        "    layer_norm,\n",
        "    mlp_block,\n",
        "    output_block,\n",
        "    add_pre_post_process,\n",
        ")\n",
        "from keras_cv_attention_models.download_and_load import reload_model_weights"
      ],
      "metadata": {
        "id": "lafS-j4zxmWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_cv_attention_models import backend\n",
        "from keras_cv_attention_models.backend import layers, models, functional, image_data_format\n",
        "from keras_cv_attention_models.attention_layers import activation_by_name, add_pre_post_process\n",
        "from keras_cv_attention_models.download_and_load import reload_model_weights\n",
        "\n",
        "BATCH_NORM_EPSILON = 1e-5\n",
        "\n",
        "def spatial_gating_block(inputs, name=None):\n",
        "    uu, vv = functional.split(inputs, 2, axis=-1 if backend.image_data_format() == \"channels_last\" else 1)\n",
        "    vv = layer_norm(vv, name=name and name + \"vv_ln\")\n",
        "    vv = layers.Permute((2, 1), name=name and name + \"permute_1\")(vv) if backend.image_data_format() == \"channels_last\" else vv\n",
        "    ww_init = initializers.truncated_normal(stddev=1e-6)\n",
        "    vv = layers.Dense(vv.shape[-1], kernel_initializer=ww_init, bias_initializer=\"ones\", name=name and name + \"vv_dense\")(vv)\n",
        "    vv = layers.Permute((2, 1), name=name and name + \"permute_2\")(vv) if backend.image_data_format() == \"channels_last\" else vv\n",
        "    gated_out = layers.Multiply()([uu, vv])\n",
        "    return gated_out\n",
        "\n",
        "def layer_norm(inputs, axis=\"auto\", name=None):\n",
        "    \"\"\"Typical LayerNormalization with epsilon=1e-5\"\"\"\n",
        "    norm_axis = (-1 if backend.image_data_format() == \"channels_last\" else 1) if axis == \"auto\" else axis\n",
        "    return layers.LayerNormalization(axis=norm_axis, epsilon=BATCH_NORM_EPSILON, name=name)(inputs)\n",
        "\n",
        "\n",
        "def mlp_block(inputs, hidden_dim, output_channel=-1, drop_rate=0, use_conv=False, use_bias=True, activation=\"gelu\", name=None):\n",
        "\n",
        "    channnel_axis = -1 if image_data_format() == \"channels_last\" else (1 if use_conv else -1)\n",
        "    output_channel = output_channel if output_channel > 0 else inputs.shape[channnel_axis]\n",
        "\n",
        "    if use_conv:\n",
        "        nn = layers.Conv2D(hidden_dim, kernel_size=1, use_bias=use_bias, name=name and name + \"Conv_0\")(inputs)\n",
        "    else:\n",
        "        nn = layers.Dense(hidden_dim, use_bias=use_bias, name=name and name + \"Dense_0\")(inputs)\n",
        "\n",
        "    nn = spatial_gating_block(nn, name=name)\n",
        "\n",
        "    nn = activation_by_name(nn, activation, name=name)\n",
        "    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn\n",
        "\n",
        "    if use_conv:\n",
        "        nn = layers.Conv2D(output_channel, kernel_size=1, use_bias=use_bias, name=name and name + \"Conv_1\")(nn)\n",
        "    else:\n",
        "\n",
        "        nn = layers.Dense(output_channel, use_bias=use_bias, name=name and name + \"Dense_1\")(nn)\n",
        "\n",
        "    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn\n",
        "    return nn\n",
        "\n",
        "\n",
        "def mixer_block(inputs, tokens_mlp_dim, channels_mlp_dim, use_bias=True, drop_rate=0, activation=\"gelu\", data_format=None, name=None):\n",
        "\n",
        "    data_format = backend.image_data_format() if data_format is None else data_format\n",
        "    norm_axis = -1 if data_format == \"channels_last\" else 1\n",
        "\n",
        "    nn = layer_norm(inputs, axis=norm_axis, name=name and name + \"LayerNorm_0\")\n",
        "    nn = layers.Permute((2, 1), name=name and name + \"permute_0\")(nn) if data_format == \"channels_last\" else nn\n",
        "    nn = mlp_block(nn, tokens_mlp_dim, use_bias=use_bias, activation=activation, name=name and name + \"token_mixing/\")\n",
        "    nn = layers.Permute((2, 1), name=name and name + \"permute_1\")(nn) if data_format == \"channels_last\" else nn\n",
        "    if drop_rate > 0:\n",
        "        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + \"token_drop\")(nn)\n",
        "    token_out = layers.Add(name=name and name + \"add_0\")([nn, inputs])\n",
        "\n",
        "    nn = layer_norm(token_out, axis=norm_axis, name=name and name + \"LayerNorm_1\")\n",
        "    nn = nn if data_format == \"channels_last\" else layers.Permute((2, 1), name=name and name + \"permute_2\")(nn)\n",
        "    nn = mlp_block(nn, channels_mlp_dim, use_bias=use_bias, activation=activation, name=name and name + \"channel_mixing/\")\n",
        "    channel_out = nn if data_format == \"channels_last\" else layers.Permute((2, 1), name=name and name + \"permute_3\")(nn)\n",
        "    if drop_rate > 0:\n",
        "        channel_out = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + \"channel_drop\")(channel_out)\n",
        "    return layers.Add(name=name and name + \"output\")([channel_out, token_out])\n",
        "\n",
        "\n",
        "def Mixer(\n",
        "    num_blocks,\n",
        "    patch_size,\n",
        "    stem_width,\n",
        "    tokens_mlp_dim,\n",
        "    channels_mlp_dim,\n",
        "    input_shape=(9, 9, 17),\n",
        "    attn_kernel_size=4,\n",
        "    attn_drop_rate=0,\n",
        "    num_heads=8,\n",
        "    num_classes=7,\n",
        "    activation=\"gelu\",\n",
        "    sam_rho=0,\n",
        "    dropout=0,\n",
        "    drop_connect_rate=0,\n",
        "    classifier_activation=\"softmax\",\n",
        "    pretrained=None,\n",
        "    model_name=\"mixer\",\n",
        "    kwargs=None,\n",
        "):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    X=inputs\n",
        "    #X=data_augmentation(inputs)\n",
        "    pos_emb1 = depthwise_conv2d_no_bias(X, kernel_size=1, padding=\"SAME\", use_bias=True)\n",
        "    pos_emb2 = depthwise_conv2d_no_bias(X, kernel_size=3, padding=\"SAME\", use_bias=True)\n",
        "    pos_emb3 = depthwise_conv2d_no_bias(X, kernel_size=5, padding=\"SAME\", use_bias=True)\n",
        "    pos_out = keras.layers.Add()([X, pos_emb1, pos_emb2, pos_emb3])\n",
        "\n",
        "    X = layers.Add()([X, pos_out])\n",
        "\n",
        "    nn = layers.Conv2D(stem_width, kernel_size=patch_size, strides=1, padding=\"VALID\", name=\"stem\")(X)\n",
        "    new_shape = [nn.shape[1] * nn.shape[2], stem_width] if backend.image_data_format() == \"channels_last\" else [stem_width, nn.shape[2] * nn.shape[3]]\n",
        "    nn = layers.Reshape(new_shape)(nn)\n",
        "\n",
        "    drop_connect_s, drop_connect_e = drop_connect_rate if isinstance(drop_connect_rate, (list, tuple)) else [drop_connect_rate, drop_connect_rate]\n",
        "\n",
        "    for ii in range(num_blocks):\n",
        "\n",
        "        name = \"{}_{}/\".format(\"MixerBlock\", str(ii))\n",
        "        block_drop_rate = drop_connect_s + (drop_connect_e - drop_connect_s) * ii / num_blocks\n",
        "        nn = mixer_block(nn, tokens_mlp_dim, channels_mlp_dim, drop_rate=block_drop_rate, activation=activation, name=name)\n",
        "    nn = layer_norm(nn, name=\"pre_head_layer_norm\")\n",
        "\n",
        "    if num_classes > 0:\n",
        "        nn = layers.GlobalAveragePooling1D()(nn)  # tf.reduce_mean(nn, axis=1)\n",
        "        if dropout > 0 and dropout < 1:\n",
        "            nn = layers.Dropout(dropout)(nn)\n",
        "        nn = layers.Dense(num_classes, dtype=\"float32\", activation=classifier_activation, name=\"head\")(nn)\n",
        "\n",
        "        model = models.Model(inputs, nn, name=model_name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "BLOCK_CONFIGS = {\n",
        "    \"s32\": {\n",
        "        \"num_blocks\": 1,\n",
        "        \"patch_size\": 4,\n",
        "        \"stem_width\": 256,\n",
        "        \"tokens_mlp_dim\": 256,\n",
        "        \"channels_mlp_dim\": 256,\n",
        "\n",
        "}\n",
        "}\n",
        "\n",
        "def SGUMLP(input_shape=(9, 9, 17), num_classes=7, activation=\"gelu\", classifier_activation=\"softmax\", pretrained=None, **kwargs):\n",
        "    return Mixer(**BLOCK_CONFIGS[\"s32\"], **locals(), model_name=\"mixer_s32\", **kwargs)\n"
      ],
      "metadata": {
        "id": "WCFtTpUUxmZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=SGUMLP(input_shape=(9, 9, 17), num_classes=7)"
      ],
      "metadata": {
        "id": "JK4dDNlmxmb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "dropout_rate = 0.4\n",
        "learning_rate = 0.001\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return average_acc\n",
        "\n",
        "\n",
        "# Define per-fold score containers\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 7\n",
        "no_epochs = 100\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "aa_per_fold = []\n",
        "oa_per_fold = []\n",
        "ki_per_fold = []\n",
        "\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "# Merge inputs and targets\n",
        "inputs = XtrainS\n",
        "targets = YtrainS\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "\n",
        "  model = SGUMLP(input_shape=(9, 9, 17), num_classes=7)\n",
        "  # Compile the model\n",
        "  # Compile the model\n",
        "  optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "  model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=100)\n",
        "\n",
        "\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(XtestS, YtestS, verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "  Y_pred = model.predict(XtestS)\n",
        "  y_pred = np.argmax(Y_pred, axis=1)\n",
        "  confusion = confusion_matrix(YtestS, y_pred)\n",
        "  oa = accuracy_score(YtestS, y_pred)\n",
        "\n",
        "  oa_per_fold.append(oa * 100)\n",
        "  aa = AA_andEachClassAccuracy(confusion)\n",
        "  aa_per_fold.append(aa * 100)\n",
        "  kappa = cohen_kappa_score(YtestS, y_pred)\n",
        "  ki_per_fold.append(kappa * 100)\n",
        "\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(f'> OA: {np.mean(oa_per_fold)} (+- {np.std(oa_per_fold)})')\n",
        "print(f'> AA: {np.mean(aa_per_fold)} (+- {np.std(aa_per_fold)})')\n",
        "print(f'> KI: {np.mean(ki_per_fold)} (+- {np.std(ki_per_fold)})')\n"
      ],
      "metadata": {
        "id": "ztS659Ipxme5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
